{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aptamer Model Training\n",
    "\n",
    "This notebook trains machine learning models for aptamer binding affinity prediction and cross-reactivity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.data_loader import AptamerDataLoader\n",
    "from src.models.binding_affinity import BindingAffinityPredictor\n",
    "from src.models.cross_reactivity import CrossReactivityAnalyzer\n",
    "from src.visualization.plot_utils import AptamerVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature-Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-enriched dataset\n",
    "feature_path = '../data/processed/aptamers_with_features.csv'\n",
    "\n",
    "if os.path.exists(feature_path):\n",
    "    df = pd.read_csv(feature_path)\n",
    "    print(f\"Loaded feature-enriched data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "else:\n",
    "    print(f\"Feature-enriched data not found at {feature_path}\")\n",
    "    print(\"Please run the 02_feature_engineering.ipynb notebook first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "data_loader = AptamerDataLoader()\n",
    "train_df, val_df, test_df = data_loader.split_data(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binding Affinity Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have binding affinity data\n",
    "binding_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'binding' in col.lower() or 'affinity' in col.lower() or 'kd' in col.lower():\n",
    "        binding_col = col\n",
    "        break\n",
    "\n",
    "if binding_col:\n",
    "    print(f\"Using '{binding_col}' as binding affinity target\")\n",
    "else:\n",
    "    print(\"No binding affinity column found. Creating a synthetic target for demonstration.\")\n",
    "    \n",
    "    # Create synthetic binding affinity for demonstration\n",
    "    # This should be replaced with real data in production\n",
    "    np.random.seed(42)\n",
    "    gc_col = 'GC_Content' if 'GC_Content' in df.columns else 'gc_content'\n",
    "    df['binding_affinity'] = (\n",
    "        0.5 * (df[gc_col] / 100) +\n",
    "        0.3 * (1 - abs(df['length'] - 30) / 30) +\n",
    "        0.2 * np.random.random(len(df))\n",
    "    )\n",
    "    binding_col = 'binding_affinity'\n",
    "    \n",
    "    # Update the training, validation, and test sets\n",
    "    train_df['binding_affinity'] = df['binding_affinity'].iloc[train_df.index].values\n",
    "    val_df['binding_affinity'] = df['binding_affinity'].iloc[val_df.index].values\n",
    "    test_df['binding_affinity'] = df['binding_affinity'].iloc[test_df.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple binding affinity models and compare them\n",
    "model_types = ['random_forest', 'gradient_boosting', 'xgboost']\n",
    "binding_models = {}\n",
    "binding_metrics = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\nTraining {model_type} model...\")\n",
    "    \n",
    "    # Configure model\n",
    "    if model_type == 'random_forest':\n",
    "        config = {'model_type': model_type, 'n_estimators': 100, 'max_depth': 10}\n",
    "    elif model_type == 'gradient_boosting':\n",
    "        config = {'model_type': model_type, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
    "    elif model_type == 'xgboost':\n",
    "        config = {'model_type': model_type, 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.05}\n",
    "    else:\n",
    "        config = {'model_type': model_type}\n",
    "    \n",
    "    # Initialize and train model\n",
    "    predictor = BindingAffinityPredictor(config)\n",
    "    metrics = predictor.train(train_df, binding_col, validation_df=val_df)\n",
    "    \n",
    "    # Store model and metrics\n",
    "    binding_models[model_type] = predictor\n",
    "    binding_metrics[model_type] = metrics\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(f\"Training MSE: {metrics.get('training_mse', 'N/A'):.6f}\")\n",
    "    print(f\"Training R²: {metrics.get('training_r2', 'N/A'):.6f}\")\n",
    "    \n",
    "    if 'validation_mse' in metrics:\n",
    "        print(f\"Validation MSE: {metrics.get('validation_mse', 'N/A'):.6f}\")\n",
    "        print(f\"Validation R²: {metrics.get('validation_r2', 'N/A'):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "model_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': model_type,\n",
    "        'Training MSE': metrics.get('training_mse', float('nan')),\n",
    "        'Training R²': metrics.get('training_r2', float('nan')),\n",
    "        'Validation MSE': metrics.get('validation_mse', float('nan')),\n",
    "        'Validation R²': metrics.get('validation_r2', float('nan'))\n",
    "    }\n",
    "    for model_type, metrics in binding_metrics.items()\n",
    "])\n",
    "\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best performing model based on validation R²\n",
    "best_model_type = model_comparison.iloc[model_comparison['Validation R²'].idxmax()]['Model']\n",
    "best_binding_model = binding_models[best_model_type]\n",
    "\n",
    "print(f\"Best binding affinity model: {best_model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_metrics = best_binding_model.evaluate_model(test_df, binding_col)\n",
    "\n",
    "print(\"Test set evaluation:\")\n",
    "print(f\"MSE: {test_metrics['mse']:.6f}\")\n",
    "print(f\"RMSE: {test_metrics['rmse']:.6f}\")\n",
    "print(f\"R²: {test_metrics['r2']:.6f}\")\n",
    "print(f\"MAE: {test_metrics['mae']:.6f}\")\n",
    "print(f\"Correlation: {test_metrics['correlation']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actual vs predicted values\n",
    "test_predictions = best_binding_model.predict(test_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(test_df[binding_col], test_predictions, alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Actual Binding Affinity')\n",
    "plt.ylabel('Predicted Binding Affinity')\n",
    "plt.title(f'Binding Affinity Prediction ({best_model_type})')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = best_binding_model.get_feature_importance()\n",
    "\n",
    "# Plot top 20 features\n",
    "top_n = min(20, len(feature_importance))\n",
    "top_features = feature_importance.sort_values('importance', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(top_features['feature'], top_features['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'Top {top_n} Features for Binding Affinity Prediction')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Reactivity Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have target information for cross-reactivity analysis\n",
    "if 'Target_Name' not in df.columns:\n",
    "    print(\"No 'Target_Name' column found. Cross-reactivity analysis requires target information.\")\n",
    "else:\n",
    "    # Train cross-reactivity model\n",
    "    print(\"Training cross-reactivity model...\")\n",
    "    \n",
    "    cross_reactivity_model = CrossReactivityAnalyzer({\n",
    "        'model_type': 'xgboost',\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.05\n",
    "    })\n",
    "    \n",
    "    cr_metrics = cross_reactivity_model.train_cross_reactivity_model(train_df, validation_df=val_df)\n",
    "    \n",
    "    # Print training metrics\n",
    "    print(f\"Training accuracy: {cr_metrics.get('training_accuracy', 'N/A'):.4f}\")\n",
    "    \n",
    "    if 'validation_accuracy' in cr_metrics:\n",
    "        print(f\"Validation accuracy: {cr_metrics.get('validation_accuracy', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Print ROC AUC scores\n",
    "    if 'training_roc_auc' in cr_metrics:\n",
    "        print(\"\\nROC AUC scores:\")\n",
    "        for target, auc_score in cr_metrics['training_roc_auc'].items():\n",
    "            if target != 'average':\n",
    "                print(f\"{target}: {auc_score:.4f}\")\n",
    "        print(f\"Average: {cr_metrics['training_roc_auc'].get('average', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cross-reactivity model on the test set\n",
    "if 'Target_Name' in df.columns:\n",
    "    # Make predictions\n",
    "    test_cr_predictions = cross_reactivity_model.predict_cross_reactivity(test_df)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(test_df['Target_Name'], test_cr_predictions['predicted_target'])\n",
    "    target_names = cross_reactivity_model.target_names\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Confusion Matrix for Target Prediction')\n",
    "    plt.xlabel('Predicted Target')\n",
    "    plt.ylabel('Actual Target')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    accuracy = (test_cr_predictions['predicted_target'] == test_df['Target_Name']).mean()\n",
    "    print(f\"Test set accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-reactivity\n",
    "if 'Target_Name' in df.columns:\n",
    "    # Identify potentially cross-reactive aptamers\n",
    "    crossreact_df = cross_reactivity_model.identify_cross_reactive_aptamers(\n",
    "        test_cr_predictions, threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Count cross-reactive aptamers\n",
    "    cross_reactive_count = crossreact_df['is_cross_reactive'].sum()\n",
    "    total_count = len(crossreact_df)\n",
    "    \n",
    "    print(f\"Found {cross_reactive_count} potentially cross-reactive aptamers out of {total_count} ({cross_reactive_count/total_count:.1%})\")\n",
    "    \n",
    "    # Show examples of cross-reactive aptamers\n",
    "    if cross_reactive_count > 0:\n",
    "        cr_examples = crossreact_df[crossreact_df['is_cross_reactive']].head(5)\n",
    "        display_cols = ['Target_Name', 'predicted_target', 'cross_reactive_targets', 'specificity_score']\n",
    "        seq_col = 'Sequence' if 'Sequence' in cr_examples.columns else 'sequence'\n",
    "        if seq_col in cr_examples.columns:\n",
    "            display_cols = [seq_col] + display_cols\n",
    "        \n",
    "        print(\"\\nExamples of cross-reactive aptamers:\")\n",
    "        display(cr_examples[display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-reactivity\n",
    "if 'Target_Name' in df.columns:\n",
    "    visualizer = AptamerVisualizer()\n",
    "    visualizer.plot_cross_reactivity_matrix(crossreact_df, output_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binding affinity model\n",
    "binding_model_path = os.path.join(model_dir, 'binding_affinity_model.pkl')\n",
    "best_binding_model.save_model(binding_model_path)\n",
    "print(f\"Binding affinity model saved to {binding_model_path}\")\n",
    "\n",
    "# Save cross-reactivity model\n",
    "if 'Target_Name' in df.columns:\n",
    "    cross_reactivity_model_path = os.path.join(model_dir, 'cross_reactivity_model.pkl')\n",
    "    cross_reactivity_model.save_model(cross_reactivity_model_path)\n",
    "    print(f\"Cross-reactivity model saved to {cross_reactivity_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Key findings from model training:\n",
    "\n",
    "1. Binding affinity prediction: [Fill in after running the notebook]\n",
    "2. Most important features for binding prediction: [Fill in after running the notebook]\n",
    "3. Cross-reactivity analysis: [Fill in after running the notebook]\n",
    "4. Most promising aptamer candidates: [Fill in after running the notebook]\n",
    "\n",
    "These models will be used to select and optimize aptamers in the following notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
