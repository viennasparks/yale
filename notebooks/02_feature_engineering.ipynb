{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aptamer Feature Engineering\n",
    "\n",
    "This notebook extracts and analyzes features from aptamer sequences for use in binding affinity and cross-reactivity prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.data_loader import AptamerDataLoader\n",
    "from src.feature_extraction.sequence_features import SequenceFeatureExtractor\n",
    "from src.feature_extraction.structure_prediction import StructurePredictor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "processed_path = '../data/processed/preprocessed_aptamers.csv'\n",
    "\n",
    "if os.path.exists(processed_path):\n",
    "    df = pd.read_csv(processed_path)\n",
    "    print(f\"Loaded preprocessed data: {len(df)} rows\")\n",
    "else:\n",
    "    # Load raw data and preprocess if necessary\n",
    "    from src.data_processing.preprocessor import AptamerPreprocessor\n",
    "    \n",
    "    data_loader = AptamerDataLoader()\n",
    "    df = data_loader.load_from_csv('../data/raw/fentanyl.csv')\n",
    "    \n",
    "    preprocessor = AptamerPreprocessor()\n",
    "    df = preprocessor.clean_data(df)\n",
    "    \n",
    "    if 'Target_Name' in df.columns:\n",
    "        df = preprocessor.normalize_target_names(df)\n",
    "    \n",
    "    print(f\"Loaded and preprocessed raw data: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sequence Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine sequence column name\n",
    "seq_col = 'Sequence' if 'Sequence' in df.columns else 'sequence'\n",
    "\n",
    "if seq_col not in df.columns:\n",
    "    print(f\"ERROR: No sequence column ('{seq_col}') found in the dataset\")\n",
    "else:\n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # Extract all features\n",
    "    print(\"Extracting sequence features...\")\n",
    "    sequence_features = feature_extractor.extract_all_features(df[seq_col].tolist())\n",
    "    \n",
    "    print(f\"Extracted {len(sequence_features.columns)} sequence features\")\n",
    "    \n",
    "    # Display the first few rows of features\n",
    "    sequence_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Structural Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seq_col in df.columns:\n",
    "    # Initialize structure predictor\n",
    "    structure_predictor = StructurePredictor()\n",
    "    \n",
    "    # Predict structures and extract features\n",
    "    print(\"Predicting structures and extracting structural features...\")\n",
    "    structure_features = structure_predictor.predict_and_analyze_structures(df[seq_col].tolist())\n",
    "    \n",
    "    print(f\"Extracted {len(structure_features.columns)} structural features\")\n",
    "    \n",
    "    # Display the first few rows of features\n",
    "    structure_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Secondary Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_structure_visualization(sequence, structure):\n",
    "    \"\"\"Create a simple visualization of the secondary structure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "    \n",
    "    # Draw sequence\n",
    "    for i, nt in enumerate(sequence):\n",
    "        ax.text(i, 0, nt, ha='center', va='center',\n",
    "               bbox=dict(boxstyle='circle', facecolor='white', edgecolor='black'))\n",
    "    \n",
    "    # Draw structure (base pairs)\n",
    "    stack = []\n",
    "    for i, char in enumerate(structure):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                j = stack.pop()\n",
    "                # Draw arc connecting the pair\n",
    "                center = (i + j) / 2\n",
    "                width = i - j\n",
    "                height = width / 2\n",
    "                ax.plot([j, i], [0.5, 0.5], 'k-', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlim(-1, len(sequence))\n",
    "    ax.set_ylim(-1, 3)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show a few example structures\n",
    "if 'predicted_structure' in structure_features.columns:\n",
    "    for i in range(min(3, len(structure_features))):\n",
    "        seq = structure_features['sequence'].iloc[i]\n",
    "        struct = structure_features['predicted_structure'].iloc[i]\n",
    "        energy = structure_features['energy'].iloc[i]\n",
    "        \n",
    "        print(f\"Sequence {i+1}:\")\n",
    "        print(f\"Sequence: {seq}\")\n",
    "        print(f\"Structure: {struct}\")\n",
    "        print(f\"Energy: {energy:.2f} kcal/mol\")\n",
    "        simple_structure_visualization(seq, struct)\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "combined_df = pd.concat([\n",
    "    df.reset_index(drop=True),\n",
    "    sequence_features.drop(columns=['sequence']).reset_index(drop=True),\n",
    "    structure_features.drop(columns=['sequence']).reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Original DataFrame: {df.shape[1]} columns\")\n",
    "print(f\"Combined DataFrame: {combined_df.shape[1]} columns\")\n",
    "print(f\"Added {combined_df.shape[1] - df.shape[1]} new feature columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between features\n",
    "numeric_columns = combined_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Limit to a subset for visualization\n",
    "important_features = [\n",
    "    'gc_content', 'length', 'purine_pyrimidine_ratio', \n",
    "    'energy', 'ensemble_diversity', 'stem_count', 'hairpin_loop_count',\n",
    "    'paired_percentage', 'unpaired_percentage'\n",
    "]\n",
    "\n",
    "# Ensure all important features exist in the DataFrame\n",
    "important_features = [f for f in important_features if f in numeric_columns]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = combined_df[important_features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(important_features[:9]):  # Limit to 9 features for the grid\n",
    "    if feature in combined_df.columns:\n",
    "        sns.histplot(combined_df[feature], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'{feature} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numeric features for PCA\n",
    "numeric_df = combined_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Remove constant columns\n",
    "numeric_df = numeric_df.loc[:, numeric_df.std() > 0]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create DataFrame with PCA results\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': pca_result[:, 0],\n",
    "    'PC2': pca_result[:, 1]\n",
    "})\n",
    "\n",
    "# Add target information if available\n",
    "if 'Target_Name' in combined_df.columns:\n",
    "    pca_df['Target'] = combined_df['Target_Name']\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(12, 8))\n",
    "if 'Target' in pca_df.columns:\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='Target', data=pca_df, s=100, alpha=0.7)\n",
    "    plt.legend(title='Target', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "else:\n",
    "    sns.scatterplot(x='PC1', y='PC2', data=pca_df, s=100, alpha=0.7)\n",
    "\n",
    "plt.title('PCA of Aptamer Features')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(f\"Explained variance ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"Total: {sum(pca.explained_variance_ratio_[:2]):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature loadings from PCA\n",
    "feature_loadings = pd.DataFrame({\n",
    "    'Feature': numeric_df.columns,\n",
    "    'PC1_loading': pca.components_[0],\n",
    "    'PC2_loading': pca.components_[1]\n",
    "})\n",
    "\n",
    "# Sort by absolute loading values\n",
    "feature_loadings['PC1_abs'] = abs(feature_loadings['PC1_loading'])\n",
    "feature_loadings['PC2_abs'] = abs(feature_loadings['PC2_loading'])\n",
    "\n",
    "# Top features for PC1\n",
    "print(\"Top features for PC1:\")\n",
    "print(feature_loadings.sort_values('PC1_abs', ascending=False).head(10)[['Feature', 'PC1_loading']])\n",
    "\n",
    "print(\"\\nTop features for PC2:\")\n",
    "print(feature_loadings.sort_values('PC2_abs', ascending=False).head(10)[['Feature', 'PC2_loading']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature loadings\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(feature_loadings['PC1_loading'], feature_loadings['PC2_loading'], alpha=0.7)\n",
    "\n",
    "# Add feature labels\n",
    "for i, txt in enumerate(feature_loadings['Feature']):\n",
    "    # Only label important features for readability\n",
    "    if (abs(feature_loadings['PC1_loading'].iloc[i]) > 0.2 or \n",
    "        abs(feature_loadings['PC2_loading'].iloc[i]) > 0.2):\n",
    "        plt.annotate(txt, \n",
    "                    (feature_loadings['PC1_loading'].iloc[i], \n",
    "                     feature_loadings['PC2_loading'].iloc[i]),\n",
    "                    fontsize=9)\n",
    "\n",
    "plt.title('PCA Feature Loadings')\n",
    "plt.xlabel('PC1 Loading')\n",
    "plt.ylabel('PC2 Loading')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G-Quadruplex Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze G-quadruplex forming potential\n",
    "if seq_col in combined_df.columns:\n",
    "    g4_scores = structure_predictor.get_g_quadruplex_propensity(combined_df[seq_col].tolist())\n",
    "    combined_df['g4_propensity'] = g4_scores\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(combined_df['g4_propensity'], kde=True, bins=20)\n",
    "    plt.title('G-Quadruplex Forming Propensity Distribution')\n",
    "    plt.xlabel('G4 Propensity Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show target-specific distribution if available\n",
    "    if 'Target_Name' in combined_df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='Target_Name', y='g4_propensity', data=combined_df)\n",
    "        plt.title('G-Quadruplex Propensity by Target')\n",
    "        plt.xlabel('Target')\n",
    "        plt.ylabel('G4 Propensity Score')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermodynamic Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thermodynamic stability\n",
    "if seq_col in combined_df.columns:\n",
    "    thermo_results = structure_predictor.calculate_thermodynamic_stability(combined_df[seq_col].tolist())\n",
    "    \n",
    "    # Extract stability scores\n",
    "    stability_scores = [result['stability_score'] for result in thermo_results]\n",
    "    tm_estimates = [result['approximated_tm'] for result in thermo_results]\n",
    "    \n",
    "    combined_df['stability_score'] = stability_scores\n",
    "    combined_df['estimated_tm'] = tm_estimates\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    sns.histplot(combined_df['stability_score'], kde=True, bins=20, ax=axes[0])\n",
    "    axes[0].set_title('Aptamer Stability Score Distribution')\n",
    "    axes[0].set_xlabel('Stability Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    sns.histplot(combined_df['estimated_tm'], kde=True, bins=20, ax=axes[1])\n",
    "    axes[1].set_title('Estimated Melting Temperature (Tm) Distribution')\n",
    "    axes[1].set_xlabel('Estimated Tm (°C)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature-Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset with all features\n",
    "output_path = '../data/processed/aptamers_with_features.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "print(f\"Feature-enriched dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Key findings from feature engineering:\n",
    "\n",
    "1. Sequence features: [Fill in after running the notebook]\n",
    "2. Structural features: [Fill in after running the notebook]\n",
    "3. Most important features: [Fill in after running the notebook]\n",
    "4. Target-specific patterns: [Fill in after running the notebook]\n",
    "\n",
    "These features will be used for training machine learning models in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
